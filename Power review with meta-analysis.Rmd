---
title: "Epilepsy Surgery Outcomes Power Review"
author: "Adam S Dickey"
date: "7/25/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(fig.width=10, fig.height=6) 


##libraries
library(readxl) #Read Excel file
library(Exact) #Exact power calculation
library(tidyverse) #Needed for pipe, other stuff
library(meta) #Meta-analysis

```
### Load raw data

I will load the data from the Excel spreadsheet and show the first 6 rows.
```{r data}

samplesize <- read_excel("Dickey Appendix S1.xlsx")
head(samplesize)
numStudies <-nrow(samplesize)
minmax <-range(samplesize$nTot) 
```
Data was exacted from `r numStudies` studies.
The median sample size is `r median(samplesize$nTot)`, ranging from `r minmax[1]` to `r minmax[2]` subjects.


### Meta-analysis
```{r fig.width=10, fig.height=5}
#Subset larger studies >N in each group
minSamp=20
sampleBig <- subset(samplesize,nA>minSamp & nB>minSamp & Exclude==0)

#Run meta-analysis
m.publ=metabin(Apos, nA, Bpos, nB, data=sampleBig, studlab=paste0(First_Author, " (", Year, ")"), method.tau = "PM", sm="OR")
forest(m.publ, sortvar=Year, prediction=TRUE, label.right = "Favours Positive", label.left = "Favours Negative")
or=exp(m.publ$TE.random)
```



I first performed a meta-analysis of `r nrow(sampleBig)` studies which had more than `r minSamp` subjects in each group.  We excluded 2 studies because their data were subsets of later larger studies.  

The pooled odds-ratio from the meta-analysis, assuming a random effects model, was `r sprintf("%.2f",or)`. 

### Clinically meaningful effect size
```{r, include=FALSE}


#Computer effect size by fixing prob(seizure free) in negative Group (B)
#Then apply odds ratio

#Access weights
w<-m.publ$w.random
w<-w/sum(w) #scale to sum to 1

pNegindivid<-sampleBig$Bpos/sampleBig$nB
pNeg<-sum(pNegindivid*w)

#given avg B group, apply odds ratio, get A group
oddsNeg<-pNeg/(1-pNeg)
oddsPos<-oddsNeg*or
pPos<-oddsPos/(1+oddsPos)

```
I computed a weighted average of the probability of seizure freedom in the negative finding group, which was `r sprintf("%.3f",pNeg)`.  That is equivalent to odds for the negative group of `r sprintf("%.2f",oddsNeg)`, which can be multiplied by the pooled odds ratio of `r sprintf("%.2f",or)` to give the odds of the positive group of `r sprintf("%.2f",oddsPos)`, or probability of seizure freedom of `r sprintf("%.3f",pPos)`.  Thus we define our clinically meaningful effect size as `r sprintf("%.1f%%",pNeg*100)` seizure free in the negative group and `r sprintf("%.1f%%",pPos*100)` seizure free in the positive group.





### Calculate power
I will compute the power for each study using the number of subjects in the positive group and number of subjects in the negative group, using the R package "Exact" and the Chi-square statistical test.

```{r}
powerVal <-rep(0,numStudies)
for (i in 1:numStudies) {
  temp <-power.exact.test(pPos,pNeg,samplesize$nA[i],samplesize$nB[i],method="pearson chisq")
  powerVal[i] <- temp$power
}
powerMedian <- median(powerVal)
moreThan50 = sum(powerVal>.5)
```
The median power is `r sprintf("%.1f%%",powerMedian*100)`.  Only `r moreThan50` of `r numStudies` studies have greater than 50% power.

### Select 90th percentile
``` {r}
samplesize2 <- cbind(samplesize,powerVal) #add power in

sorted <- sort(powerVal,decreasing=TRUE,index.return=TRUE) #sort from high power to low

#t50 <-samplesize2[sorted$ix[1:moreThan50],]
t50 <-samplesize2[sorted$ix[1:round(numStudies/10)],]

#Compute seizure free % for positive and negative finding
#Use sprintf to keep one decimal place, add % sign
szFreePos <- sprintf("%.1f%%",t50$Apos/t50$nA*100)
szFreeNeg <- sprintf("%.1f%%",t50$Bpos/t50$nB*100)
powerNice <- sprintf("%.1f%%",t50$powerVal*100)

dataNice <- data.frame(t50$First_Author,t50$Year,t50$nTot,powerNice,t50$Exposure,t50$Outcome,szFreePos,szFreeNeg)
names(dataNice) <- c('Author', 'Year','N','Power','Finding','Outcome','%SR Pos', '%SR Neg')
knitr::kable(dataNice,align=c('l','l','c','c','l','l','c'))
```
**Table 1: Selected SEEG studies in the 90th percentile for power.**  Power was calculated using a Chi-square test, assuming a difference in seizure freedom of `r sprintf("%.1f%%",pPos*100)` in group 1 and `r sprintf("%.1f%%",pNeg*100)` in group 2 (Odds Ratio `r sprintf("%.2f",or)`).   Only `r moreThan50` of 69 studies had more than 50% power.  Abbreviations include sample size (N), % of patients with seizure reduction in the positive (SR+) or negative group (SR-), magnetic resonance imaging (MRI), low frequency stimulation (LFS), electrocorticography (ECOG), radiofrequency thermocoagulation (RF-TC)  .


## Power vs. Sample Size plot

I will calculate the power for a range of sample sizes, assuming 2:1 allocation
```{r}
#Create data for plot
nMax <- 100
n1 <-c(1:nMax)
n2 <- 2*n1
nTot <- n1+n2

powerSampleSize <-rep(0,nMax)
for (i in 1:nMax) {
  temp <-power.exact.test(pPos,pNeg,n2[i],n1[i],method="pearson chisq")
  powerSampleSize[i] <- temp$power
}
```

Plotting code is hidden but included in the Rmd file
```{r, echo=FALSE}
#plot(nTot,powerSampleSize)
matplot(nTot,powerSampleSize*100,type = "l",xlab="Sample Size (N)",ylab="% Power",ylim=c(0,100),col='red')

ind80 <- which(powerSampleSize>.8)
ind <-ind80[1]
x <-c(nTot[ind],nTot[ind],0)
y <-c(0,powerSampleSize[ind],powerSampleSize[ind])
matplot(x,y*100,2,type = "l",add=TRUE)
points(x[2],y[2]*100)
text(160,90,'Well-powered study')


indMed <- which(powerSampleSize>powerMedian)
ind2 <- indMed[1]
x2 <-c(nTot[ind2],nTot[ind2],0)
y2 <-c(0,powerSampleSize[ind2],powerSampleSize[ind2])
matplot(x2,y2*100,3,type = "l",add=TRUE,col='blue')
points(x2[2],y2[2]*100,col='blue')
text(70,20,'Median SEEG study',col='blue')

```

**Figure 1: Power as a function of sample size.**  Assuming 2:1 allocation, a study would need `r nTot[ind]` subjects (`r n2[ind]` vs. `r n1[ind]`) to have 80% statistical power to detect a difference in seizure freedom of `r sprintf("%.1f%%",pPos*100)`in group 1 and `r sprintf("%.1f%%",pNeg*100)` in group 2.  However, the median SEEG study only has around `r sprintf("%.0f%%",powerMedian*100)` power to detect this difference.  For 2:1 allocation, this corresponds to a sample size of `r nTot[ind2]` (`r n2[ind2]` vs. `r n1[ind2]`).

The exact power calculation does not increase monotonically with increasing sample size. This is likely because the Chi-square test relies on a Gaussian approximation which doesn't apply for small sample sizes, and for some values the percentages fall in between the discrete bins.